{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtClrVvZTI6c",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 5   \n",
        "## Name: Akram Shaik   \n",
        "## Z-ID: Z1857655"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKa9dDTRy8K2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc4GR0VvzxZX",
        "colab_type": "code",
        "outputId": "0f5e66a1-ae12-4389-aa16-0545f75f549d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# uploading input data locally\n",
        "'''from google.colab import files\n",
        "uploaded = files.upload()\n",
        "import io\n",
        "reddit_train = pd.read_csv(io.BytesIO(uploaded['reddit_200k_train.csv']), encoding='unicode_escape')\n",
        "reddit_test = pd.read_csv(io.BytesIO(uploaded['reddit_200k_test.csv']), encoding='unicode_escape')\n",
        "'''"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"from google.colab import files\\nuploaded = files.upload()\\nimport io\\nreddit_train = pd.read_csv(io.BytesIO(uploaded['reddit_200k_train.csv']), encoding='unicode_escape')\\nreddit_test = pd.read_csv(io.BytesIO(uploaded['reddit_200k_test.csv']), encoding='unicode_escape')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aHUu7EH7wSb",
        "colab_type": "code",
        "outputId": "cdeecf6b-4bd5-4328-bcd1-6257c1b6a8ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#uploading train and test data from github link\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/akramshaik2326/sample/master/reddit_200k_train.csv\", encoding='latin1')\n",
        "df1 = pd.read_csv(\"https://raw.githubusercontent.com/akramshaik2326/sample/master/reddit_200k_test.csv\", encoding='latin1')\n",
        "#printing shapes of both datasets\n",
        "print(df.shape, df1.shape)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(167529, 8) (55843, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuLCzN--53YT",
        "colab_type": "code",
        "outputId": "63a9eeaf-f4b8-4c16-f185-fed14ec5386e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#X_train = pd.DataFrame(df['body'])\n",
        "#y_train = pd.DataFrame(df['REMOVED'])\n",
        "#X_test = pd.DataFrame(df1['body'])\n",
        "#y_test = pd.DataFrame(df1['REMOVED'])\n",
        "\n",
        "# extracting required columns from the dataset in labels only body columns is required and target is the removed column\n",
        "X_train, y_train = df.body, df.REMOVED\n",
        "X_test, y_test = df1.body, df1.REMOVED\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(167529,) (167529,)\n",
            "(55843,) (55843,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jMcxTxMQJK3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "93041fd8-92dc-4bc2-c7f0-5d102bf8e8b3"
      },
      "source": [
        "X_test.dtype"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('O')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5zslXRhP1vL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "d9cf75ae-4e3b-4778-95f1-c9d35da7be26"
      },
      "source": [
        "y_train.head()"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    False\n",
              "1     True\n",
              "2     True\n",
              "3    False\n",
              "4    False\n",
              "Name: REMOVED, dtype: bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIGL9KDb6UU-",
        "colab_type": "code",
        "outputId": "f0dba1a9-23ae-46f5-f739-33c63780143f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "#printing content of X_train whiuch contains tweets\n",
        "X_train.head()"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    I've always been taught it emerged from the ea...\n",
              "1    As an ECE, my first feeling as \"HEY THAT'S NOT...\n",
              "2    Monday: Drug companies stock dives on good new...\n",
              "3    i learned that all hybrids are unfertile i won...\n",
              "4    Well i was wanting to get wasted tonight.  Not...\n",
              "Name: body, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwfPcKmpYQIu",
        "colab_type": "code",
        "outputId": "7aa55309-986b-4613-9b33-878260f77201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# applying label encoder on target dataset as the values are in text form\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le.fit(y_train)\n",
        "y_train_enc = le.transform(y_train)\n",
        "y_test_enc = le.transform(y_test)\n",
        "print(y_train_enc.shape, y_train_enc)\n",
        "print(y_test_enc.shape, y_test_enc)"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(167529,) [0 1 1 ... 0 1 0]\n",
            "(55843,) [1 0 0 ... 1 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbNiHNB8unKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing countvectorizer from sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#creating object of countvectorizer with pareameter stopwords english as it will remove all stopwords from tweets\n",
        "vect = CountVectorizer(stop_words='english')\n",
        "\n",
        "#fitting transforming the train and test data to create bag of words model\n",
        "bow_train = vect.fit_transform(X_train)\n",
        "bow_test  = vect.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1GmqDn4wel3",
        "colab_type": "code",
        "outputId": "ff3144d6-1d01-46df-f4de-1f2f86b9a89b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#printing shapes of bag of words of train and test data\n",
        "print(bow_test.shape, bow_train.shape)"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(55843, 113494) (167529, 113494)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcnOnDoHwiEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing logistic regression to apply classification \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# using gridsearchcv to tune the classificatiuon model with right value of regularization\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Assignment list of c values to find the one that gives best results\n",
        "# not large range of values are added in C and not many parmeters have been given as the dataset is huge it will take a large ammount of time to execute\n",
        "grid = {\"C\" : [0.001,0.01,0.1, 0.5, 1] }\n",
        "\n",
        "#creating model instance\n",
        "logreg = LogisticRegression(class_weight='balanced', random_state=1)\n",
        "#implementing gridsearch with 10 folds\n",
        "logreg_cv = GridSearchCV( logreg, grid, cv = 10 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDcIGpULyu4b",
        "colab_type": "code",
        "outputId": "673ea9b0-207e-4deb-b815-488d53a80b52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# fitting bag of words on model\n",
        "logreg_cv.fit(bow_train, y_train_enc)\n",
        "#printing the best parameters and accuracy\n",
        "print(\"tuned hpyerparameters : (best parameters) \", logreg_cv.best_params_)\n",
        "print(\" Train data accuracy : \", logreg_cv.best_score_)"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tuned hpyerparameters : (best parameters)  {'C': 0.1}\n",
            " Train data accuracy :  0.6707853579917347\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2VVdPzSzgUZ",
        "colab_type": "code",
        "outputId": "d5183be6-b528-4334-e723-ce57ede12431",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# printing accuracy of model on test data\n",
        "print(\"Test data accuracy : \", logreg_cv.score(bow_test, y_test_enc))"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test data accuracy :  0.6719373959135433\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZqmUTcaJzq5",
        "colab_type": "code",
        "outputId": "3ab5d1ac-f9fb-4367-be64-e3ed15253d0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "# finding or predicting target y values on test data\n",
        "y_pred = logreg_cv.predict( bow_test)\n",
        "\n",
        "# finding confusion matrix and classification report of the model's prediction based on actual and predicted values\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(\" Confusion Matrix : \\n\", confusion_matrix( y_test_enc, y_pred ) )\n",
        "print('\\n')\n",
        "print(\"=== Classification Report ===\")\n",
        "print(classification_report( y_test_enc, y_pred ) )\n",
        "\n",
        "# finding average f1 score of the model\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"f1_score of logistic regression on scaled data : {:.3f}\".format( f1_score( y_test_enc, y_pred ) ) )"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Confusion Matrix : \n",
            " [[20464 14101]\n",
            " [ 4219 17059]]\n",
            "\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.59      0.69     34565\n",
            "           1       0.55      0.80      0.65     21278\n",
            "\n",
            "    accuracy                           0.67     55843\n",
            "   macro avg       0.69      0.70      0.67     55843\n",
            "weighted avg       0.72      0.67      0.68     55843\n",
            "\n",
            "f1_score of logistic regression on scaled data : 0.651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzRMgO2Y0ki2",
        "colab_type": "text"
      },
      "source": [
        "# ***Question: 2***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE9Mz3qbujFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using vect4 instance which is based on value of min df that is 4,on char analyzer with ngram range (1,1)\n",
        "# different min df values were evaluated but 4 gave best results\n",
        "vect4 = CountVectorizer(min_df=4, stop_words='english', token_pattern=r\"\\b\\w[\\w’]+\\b\", ngram_range=(1, 1), analyzer='char_wb')\n",
        "bow_train4 = vect4.fit_transform(X_train)\n",
        "bow_test4 = vect4.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpMWZffRzEqL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#applyig tfidf on the bow model\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "#from sklearn.pipeline import make_pipeline\n",
        "X_train_tfidf = TfidfTransformer().fit_transform(bow_train4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJdZ5WmMzE7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test_tfidf = TfidfTransformer().fit_transform(bow_test4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1TPRVAtzJ0k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "9cdcdd82-e26b-4e39-9af8-0d7148aa137d"
      },
      "source": [
        "logreg_cv.fit(X_train_tfidf, y_train_enc)\n",
        "#printing the best parameters and accuracy\n",
        "print(\"tuned hpyerparameters : (best parameters) \", logreg_cv.best_params_)\n",
        "print(\" Train data accuracy : \", logreg_cv.best_score_)\n",
        "\n",
        "# printing accuracy of model on test data\n",
        "print(\"Test data accuracy : \", logreg_cv.score(X_test_tfidf, y_test_enc))"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tuned hpyerparameters : (best parameters)  {'C': 1}\n",
            " Train data accuracy :  0.6602558749260492\n",
            "Test data accuracy :  0.6608885625772254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVMEnypKzKAe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "39291dd7-5786-4d0d-81d9-61159b4ca5fe"
      },
      "source": [
        "# finding or predicting target y values on test data\n",
        "y_pred = logreg_cv.predict( X_test_tfidf)\n",
        "\n",
        "# finding confusion matrix and classification report of the model's prediction based on actual and predicted values\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(\" Confusion Matrix : \\n\", confusion_matrix( y_test_enc, y_pred ) )\n",
        "print('\\n')\n",
        "print(\"=== Classification Report ===\")\n",
        "print(classification_report( y_test_enc, y_pred ) )\n",
        "\n",
        "# finding average f1 score of the model\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"f1_score of logistic regression on scaled data : {:.3f}\".format( f1_score( y_test_enc, y_pred ) ) )"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Confusion Matrix : \n",
            " [[22721 11844]\n",
            " [ 7093 14185]]\n",
            "\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.66      0.71     34565\n",
            "           1       0.54      0.67      0.60     21278\n",
            "\n",
            "    accuracy                           0.66     55843\n",
            "   macro avg       0.65      0.66      0.65     55843\n",
            "weighted avg       0.68      0.66      0.67     55843\n",
            "\n",
            "f1_score of logistic regression on scaled data : 0.600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLzOn8dp4YEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#vect4 = CountVectorizer(min_df=4, stop_words='english', token_pattern=r\"\\b\\w[\\w’]+\\b\", ngram_range=(1, 3), analyzer='char_wb')\n",
        "# In char analyzer with ngram range (1,3) execution time was very high and the results were not better than ngram range (1,2)\n",
        "# so we finally consider comparing word and char analyer with ngram range in char as (1,2)  \n",
        "vect4 = CountVectorizer(min_df=4, stop_words='english', token_pattern=r\"\\b\\w[\\w’]+\\b\", ngram_range=(1, 2), analyzer='char_wb')\n",
        "bow_train4 = vect4.fit_transform(X_train)\n",
        "bow_test4 = vect4.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtXrPhP6PoqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing tfidf transformer from sklearn\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "# fitting and transformig bag of words on tfidf \n",
        "X_train_tfidf = TfidfTransformer().fit_transform(bow_train4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn1tloFKq1jj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fitting and transforming test data bag of words on tfidf\n",
        "X_test_tfidf = TfidfTransformer().fit_transform(bow_test4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avwZwRob7XbV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "c533815d-9ba0-4d35-c965-cf930e907373"
      },
      "source": [
        "# fitting tfidf applied vectors on logistic regression\n",
        "logreg_cv.fit(X_train_tfidf, y_train_enc)\n",
        "#printing the best parameters and accuracy\n",
        "print(\"tuned hpyerparameters : (best parameters) of tfidf applied BoW \", logreg_cv.best_params_)\n",
        "print(\" Train data accuracy of tfidf : \", logreg_cv.best_score_)\n",
        "\n",
        "# printing accuracy of model on test data\n",
        "print(\"Test data accuracy : \", logreg_cv.score(X_test_tfidf, y_test_enc))"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tuned hpyerparameters : (best parameters) of tfidf applied BoW  {'C': 1}\n",
            " Train data accuracy of tfidf :  0.6919041125565559\n",
            "Test data accuracy :  0.6933724907329477\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdYvQI9iqqJI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "99378468-3de9-4b05-aa53-0a46df1a75fd"
      },
      "source": [
        "# finding or predicting target y values on test data\n",
        "y_pred = logreg_cv.predict( X_test_tfidf)\n",
        "\n",
        "# finding confusion matrix and classification report of the model's prediction based on actual and predicted values\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(\" Confusion Matrix : \\n\", confusion_matrix( y_test_enc, y_pred ) )\n",
        "print('\\n')\n",
        "print(\"=== Classification Report ===\")\n",
        "print(classification_report( y_test_enc, y_pred ) )\n",
        "\n",
        "# finding average f1 score of the model\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"f1_score of logistic regression on scaled data : {:.3f}\".format( f1_score( y_test_enc, y_pred ) ) )"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Confusion Matrix : \n",
            " [[24011 10554]\n",
            " [ 6569 14709]]\n",
            "\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.69      0.74     34565\n",
            "           1       0.58      0.69      0.63     21278\n",
            "\n",
            "    accuracy                           0.69     55843\n",
            "   macro avg       0.68      0.69      0.68     55843\n",
            "weighted avg       0.71      0.69      0.70     55843\n",
            "\n",
            "f1_score of logistic regression on scaled data : 0.632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnV1odrW0CC5",
        "colab_type": "text"
      },
      "source": [
        "# Word analyzer   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfGOwI6F2Qeo",
        "colab_type": "text"
      },
      "source": [
        "### First analyzing on min df -2 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3ejzzvgAqOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vect2 = CountVectorizer(min_df=2, stop_words='english', token_pattern=r\"\\b\\w[\\w’]+\\b\")\n",
        "bow_train2 = vect2.fit_transform(X_train)\n",
        "bow_test2 = vect2.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOZMQFT_49wP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "#from sklearn.pipeline import make_pipeline\n",
        "X_train_tfidf = TfidfTransformer().fit_transform(bow_train2)\n",
        "X_test_tfidf = TfidfTransformer().fit_transform(bow_test2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyfcWTe2L_qc",
        "colab_type": "code",
        "outputId": "d68e3667-5b20-47d9-af90-c343bacd92eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "logreg_cv.fit(X_train_tfidf, y_train)\n",
        "#printing the best parameters and accuracy\n",
        "print(\"tuned hpyerparameters : (best parameters) \", logreg_cv.best_params_)\n",
        "print(\" Train data accuracy : \", logreg_cv.best_score_)\n",
        "\n",
        "# printing accuracy of model on test data\n",
        "print(\"Test data accuracy : \", logreg_cv.score(X_test_tfidf, y_test_enc))"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tuned hpyerparameters : (best parameters)  {'C': 1}\n",
            " Train data accuracy :  0.6872422053618987\n",
            "Test data accuracy :  0.6893254302240209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idaGLKj0MS36",
        "colab_type": "code",
        "outputId": "047400af-8610-4e90-da2e-1bf77f55fed4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "# finding or predicting target y values on test data\n",
        "y_pred = logreg_cv.predict( X_test_tfidf)\n",
        "\n",
        "# finding confusion matrix and classification report of the model's prediction based on actual and predicted values\n",
        "print(\" Confusion Matrix : \\n\", confusion_matrix( y_test_enc, y_pred ) )\n",
        "print('\\n')\n",
        "print(\"=== Classification Report ===\")\n",
        "print(classification_report( y_test_enc, y_pred ) )\n",
        "\n",
        "# finding average f1 score of the model\n",
        "print(\"f1_score of logistic regression on scaled data : {:.3f}\".format( f1_score( y_test_enc, y_pred ) ) )"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Confusion Matrix : \n",
            " [[22721 11844]\n",
            " [ 5505 15773]]\n",
            "\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.66      0.72     34565\n",
            "           1       0.57      0.74      0.65     21278\n",
            "\n",
            "    accuracy                           0.69     55843\n",
            "   macro avg       0.69      0.70      0.68     55843\n",
            "weighted avg       0.72      0.69      0.69     55843\n",
            "\n",
            "f1_score of logistic regression on scaled data : 0.645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcFSTYOy2bGk",
        "colab_type": "text"
      },
      "source": [
        "### Second analyzing with min df -4 and applying stemming with ngram range 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4MQzTtMB6IB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# applying stemming on train and test data to check if this helps increasing the accuracy\n",
        "X_train1 = pd.DataFrame(df['body'])\n",
        "X_test1 = pd.DataFrame(df1['body'])\n",
        "# importing porterstemmer from nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "#applying stemmer on train data\n",
        "for i in X_train1:\n",
        "    i = [porter.stem(word) for word in i]\n",
        "#applying stemmer on test data\n",
        "for i in X_test1:\n",
        "    i = [porter.stem(word) for word in i]\n",
        "\n",
        "X_train1 = X_train1.body\n",
        "X_test1 = X_test1.body"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twkX2BNcIc1h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3165b2e8-52c0-4dd7-cfa4-20e9de23749b"
      },
      "source": [
        "print(X_train1.shape, X_test1.shape)"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(167529,) (55843,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bs9_mMsm5ojk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vect4 = CountVectorizer(min_df=4, stop_words='english', token_pattern=r\"\\b\\w[\\w’]+\\b\", ngram_range=(1, 2))\n",
        "bow_train4 = vect4.fit_transform(X_train1)\n",
        "bow_test4 = vect4.transform(X_test1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD6o8HOW5pXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "#from sklearn.pipeline import make_pipeline\n",
        "X_train_tfidf = TfidfTransformer().fit_transform(bow_train4)\n",
        "X_test_tfidf = TfidfTransformer().fit_transform(bow_test4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLBPXBKD5tdQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "a713f11f-487f-43bf-a1b7-07f6105c3682"
      },
      "source": [
        "logreg_cv.fit(X_train_tfidf, y_train_enc)\n",
        "#printing the best parameters and accuracy\n",
        "print(\"tuned hpyerparameters : (best parameters) \", logreg_cv.best_params_)\n",
        "print(\" Train data accuracy : \", logreg_cv.best_score_)\n",
        "\n",
        "# printing accuracy of model on test data\n",
        "print(\"Test data accuracy : \", logreg_cv.score(X_test_tfidf, y_test_enc))"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tuned hpyerparameters : (best parameters)  {'C': 1}\n",
            " Train data accuracy :  0.6913310256310706\n",
            "Test data accuracy :  0.6947334491341798\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FatRYE_F5xZ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "573cd128-d1b3-4573-fcbc-ce30e69d22d2"
      },
      "source": [
        "# finding or predicting target y values on test data\n",
        "y_pred = logreg_cv.predict( X_test_tfidf)\n",
        "\n",
        "# finding confusion matrix and classification report of the model's prediction based on actual and predicted values\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(\" Confusion Matrix : \\n\", confusion_matrix( y_test_enc, y_pred ) )\n",
        "print('\\n')\n",
        "print(\"=== Classification Report ===\")\n",
        "print(classification_report( y_test_enc, y_pred ) )\n",
        "\n",
        "# finding average f1 score of the model\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"f1_score of logistic regression on scaled data : {:.3f}\".format( f1_score( y_test_enc, y_pred ) ) )"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Confusion Matrix : \n",
            " [[23253 11312]\n",
            " [ 5735 15543]]\n",
            "\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.67      0.73     34565\n",
            "           1       0.58      0.73      0.65     21278\n",
            "\n",
            "    accuracy                           0.69     55843\n",
            "   macro avg       0.69      0.70      0.69     55843\n",
            "weighted avg       0.72      0.69      0.70     55843\n",
            "\n",
            "f1_score of logistic regression on scaled data : 0.646\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crxiEH-oce-8",
        "colab_type": "text"
      },
      "source": [
        "We can notice that word analyzer with ngram range (1,2) gave us the best test accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqTKGYvj595Z",
        "colab_type": "text"
      },
      "source": [
        "# Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cprpK2W3Efn",
        "colab_type": "text"
      },
      "source": [
        "### Creating new X_train for further modifications with new features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy0q_4SV5BHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating new dataframes\n",
        "X_train_new = pd.DataFrame(df['body'])\n",
        "X_test_new = pd.DataFrame(df1['body'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVLgZSty6CfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing re to apply regular expressions\n",
        "import re\n",
        "# this line will find any html link present in the comment and keep count as 1 or 0 of not found\n",
        "X_train_new['html_link'] = X_train_new['body'].apply(lambda x: 1 if(re.findall(r\"http\\S+\", x)) else 0)\n",
        "# same to check any hashtags\n",
        "X_train_new['hashtag'] = X_train_new['body'].apply(lambda x: 1 if(re.findall(r'\\B#\\w*[a-zA-Z]+\\w*', x)) else 0)\n",
        "# this line will create length column which has value of each comments length\n",
        "X_train_new['length'] = [len(i) for i in X_train_new['body']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_YAEEzLDmGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the above functions are aplied also on test data\n",
        "X_test_new['html_link'] = X_test_new['body'].apply(lambda x: 1 if(re.findall(r\"http\\S+\", x)) else 0)\n",
        "X_test_new['hashtag'] = X_test_new['body'].apply(lambda x: 1 if(re.findall(r'\\B#\\w*[a-zA-Z]+\\w*', x)) else 0)\n",
        "X_test_new['length'] = [len(i) for i in X_test_new['body']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY3Ywcuf6GMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating a list of punctuations\n",
        "punctuations = '''!()-![]{};:+'\"\\,<>./?@#$%^&*_~'''\n",
        "# creatin empty columns for punctuation count, capscount, and other character count if not punctuation\n",
        "X_train_new['Punc-count'] = ''\n",
        "X_train_new['CAPS-count'] = ''\n",
        "X_train_new['Other-Chars'] = ''\n",
        "for i in range(len(X_train_new)):\n",
        "  # creating counter for each column\n",
        "  punc_c = 0\n",
        "  cap_c = 0\n",
        "  other_c = 0\n",
        "  tweet = X_train_new['body'][i]\n",
        "  for word in tweet:\n",
        "    for char in word:\n",
        "      if char in punctuations:# checks if char is in punctuations\n",
        "        punc_c += 1\n",
        "      elif not char.isalpha() and char != '\\n' and not char.isdigit() and char != ' ':#checks if char is not punctuation is not a new line or space or alphabet\n",
        "        other_c += 1 \n",
        "      if char.isupper(): # checks if char is in upper case or not\n",
        "        cap_c += 1\n",
        "  X_train_new['Punc-count'][i] = punc_c\n",
        "  X_train_new['CAPS-count'][i] = cap_c\n",
        "  X_train_new['Other-Chars'][i] = other_c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qU1e1FVLD1cf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating same set of columns and precedure on test data as applyiied on train dataset\n",
        "X_test_new['Punc-count'] = ''\n",
        "X_test_new['CAPS-count'] = ''\n",
        "X_test_new['Other-Chars'] = ''\n",
        "for i in range(len(X_test_new)):\n",
        "  punc_c = 0\n",
        "  cap_c = 0\n",
        "  other_c = 0\n",
        "  tweet = X_test_new['body'][i]\n",
        "  for word in tweet:\n",
        "    for char in word:\n",
        "      if char in punctuations:# checks if char is in punctuations\n",
        "        punc_c += 1\n",
        "      elif not char.isalpha() and char != '\\n' and not char.isdigit() and char != ' ':#checks if char is not punctuation is not a new line or space or alphabet\n",
        "        other_c += 1 \n",
        "      if char.isupper(): # checks if char is in upper case or not\n",
        "        cap_c += 1\n",
        "  X_test_new['Punc-count'][i] = punc_c\n",
        "  X_test_new['CAPS-count'][i] = cap_c\n",
        "  X_test_new['Other-Chars'][i] = other_c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHBwisyhKZ5t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72b7638d-2320-4c89-d93f-53d3964df381"
      },
      "source": [
        "#printing shapes of data with new features\n",
        "print(X_train_new.shape, X_test_new.shape)"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(167529, 7) (55843, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxmKeTcOlOfU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "ce95931d-69a5-406a-92af-fb82a9a83207"
      },
      "source": [
        "X_train_new.head()"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>body</th>\n",
              "      <th>html_link</th>\n",
              "      <th>hashtag</th>\n",
              "      <th>length</th>\n",
              "      <th>Punc-count</th>\n",
              "      <th>CAPS-count</th>\n",
              "      <th>Other-Chars</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I've always been taught it emerged from the ea...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>As an ECE, my first feeling as \"HEY THAT'S NOT...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>229</td>\n",
              "      <td>10</td>\n",
              "      <td>17</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Monday: Drug companies stock dives on good new...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i learned that all hybrids are unfertile i won...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>139</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Well i was wanting to get wasted tonight.  Not...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>84</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                body  ...  Other-Chars\n",
              "0  I've always been taught it emerged from the ea...  ...            0\n",
              "1  As an ECE, my first feeling as \"HEY THAT'S NOT...  ...            2\n",
              "2  Monday: Drug companies stock dives on good new...  ...            0\n",
              "3  i learned that all hybrids are unfertile i won...  ...            0\n",
              "4  Well i was wanting to get wasted tonight.  Not...  ...            0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tIy3GQY6S2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# applying countvec with mindf 4 and ngram range 2\n",
        "vect4 = CountVectorizer(min_df=4, stop_words='english', token_pattern=r\"\\b\\w[\\w’]+\\b\", ngram_range=(1, 2))\n",
        "#bow_train4 = vect4.fit_transform(X_train_new)\n",
        "#bow_test4 = vect4.transform(X_test_new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_Xo11Xbr30p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "outputId": "0747e9ca-6bd6-4518-dd4b-11d18fd4f35d"
      },
      "source": [
        "# importing column transformer to apply countvec on new featutres dataset\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "# importing pioeline to apply each model\n",
        "from sklearn.pipeline import make_pipeline\n",
        "column_trans1 = make_column_transformer((vect4, 'body'), remainder='passthrough')\n",
        "#column_trans1.fit_transform(X_train_new)\n",
        "#from sklearn.decomposition import TruncatedSVD\n",
        "#lsa = TruncatedSVD(n_components=100, random_state=1)\n",
        "#from sklearn.preprocessing import MaxAbsScaler\n",
        "#scaler = MaxAbsScaler()\n",
        "#pipe = make_pipeline(column_trans1,scaler, lsa, logreg_cv)\n",
        "pipe = make_pipeline(column_trans1, TfidfTransformer(), logreg_cv)\n",
        "#fitting new data on all models\n",
        "pipe.fit(X_train_new, y_train_enc)"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('columntransformer',\n",
              "                 ColumnTransformer(n_jobs=None, remainder='passthrough',\n",
              "                                   sparse_threshold=0.3,\n",
              "                                   transformer_weights=None,\n",
              "                                   transformers=[('countvectorizer',\n",
              "                                                  CountVectorizer(analyzer='word',\n",
              "                                                                  binary=False,\n",
              "                                                                  decode_error='strict',\n",
              "                                                                  dtype=<class 'numpy.int64'>,\n",
              "                                                                  encoding='utf-8',\n",
              "                                                                  input='content',\n",
              "                                                                  lowercase=True,\n",
              "                                                                  max_df=1.0,\n",
              "                                                                  max_features=No...\n",
              "                                                           dual=False,\n",
              "                                                           fit_intercept=True,\n",
              "                                                           intercept_scaling=1,\n",
              "                                                           l1_ratio=None,\n",
              "                                                           max_iter=100,\n",
              "                                                           multi_class='auto',\n",
              "                                                           n_jobs=None,\n",
              "                                                           penalty='l2',\n",
              "                                                           random_state=1,\n",
              "                                                           solver='lbfgs',\n",
              "                                                           tol=0.0001,\n",
              "                                                           verbose=0,\n",
              "                                                           warm_start=False),\n",
              "                              iid='deprecated', n_jobs=None,\n",
              "                              param_grid={'C': [0.001, 0.01, 0.1, 0.5, 1]},\n",
              "                              pre_dispatch='2*n_jobs', refit=True,\n",
              "                              return_train_score=False, scoring=None,\n",
              "                              verbose=0))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uXbiFhTj9Ia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predicting test data\n",
        "predicted = pipe.predict(X_test_new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-4V-fHNzSms",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9feb2096-7220-45ae-d379-086771cfab70"
      },
      "source": [
        "X_test_new.shape"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55843, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JOc6_rmzOtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = pd.DataFrame(predicted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfbVAQxDlVY6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "14d584c2-d4de-4e0b-8a39-e373d4af547d"
      },
      "source": [
        "print(\" Confusion Matrix : \\n\", confusion_matrix( y_test_enc, y ) )\n",
        "print('\\n')\n",
        "print(\"=== Classification Report ===\")\n",
        "print(classification_report( y_test_enc, y ) )\n",
        "\n",
        "# finding average f1 score of the model\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"f1_score of logistic regression on scaled data : {:.3f}\".format( f1_score( y_test_enc, y ) ) )"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Confusion Matrix : \n",
            " [[25819  8746]\n",
            " [ 8591 12687]]\n",
            "\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.75      0.75     34565\n",
            "           1       0.59      0.60      0.59     21278\n",
            "\n",
            "    accuracy                           0.69     55843\n",
            "   macro avg       0.67      0.67      0.67     55843\n",
            "weighted avg       0.69      0.69      0.69     55843\n",
            "\n",
            "f1_score of logistic regression on scaled data : 0.594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMwmw-cPmGCe",
        "colab_type": "text"
      },
      "source": [
        "### As we can see we have not got good f1 score with new features added, those features didnt help us"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyTIGMtgGg4I",
        "colab_type": "text"
      },
      "source": [
        "# Question 4\n",
        "### implementing wordvec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vlNLwFM7KS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing word_tokenize from nltk\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwF9UWKa7KY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating empty list of tweets\n",
        "tweet_lines_train = list()\n",
        "# adding each tweet into a list\n",
        "lines_train = df['body'].values.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKAlQuSiXjes",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "edf3bec8-6800-467c-da5a-45ada8db94e2"
      },
      "source": [
        "print(pd.DataFrame(lines_train))"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                        0\n",
            "0       I've always been taught it emerged from the ea...\n",
            "1       As an ECE, my first feeling as \"HEY THAT'S NOT...\n",
            "2       Monday: Drug companies stock dives on good new...\n",
            "3       i learned that all hybrids are unfertile i won...\n",
            "4       Well i was wanting to get wasted tonight.  Not...\n",
            "...                                                   ...\n",
            "167524  Eventually they're going to use crispr to make...\n",
            "167525                  No wonder why its useless for me \n",
            "167526  Meditation brings wisdom, lack of meditation l...\n",
            "167527                So india needs to make more saffron\n",
            "167528  Is this system unique to primates? The title o...\n",
            "\n",
            "[167529 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVWf282-_vr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating same list for test data\n",
        "tweet_lines_test = list()\n",
        "lines_test = df1['body'].values.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF-ICgJ2z-iB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "56bc1c0e-910b-4640-c634-4beb4a45855c"
      },
      "source": [
        "# downloading nltk punkt and stopwords packages\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cU1w3O97Kdr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing dowloaded stopwords of nltk library\n",
        "from nltk.corpus import stopwords \n",
        "import string\n",
        "for line in lines_train:\n",
        "  tokens = word_tokenize(line) #tokenizing the words\n",
        "  tokens = [w.lower() for w in tokens] # converting all words to lower case\n",
        "  punc = str.maketrans('', '', string.punctuation) # converting punctuations to none\n",
        "  stripped = [word.translate(punc) for word in tokens] # stripping words\n",
        "  words = [ word for word in stripped if word.isalpha()] # checking if word is alphabets or not\n",
        "  stop_words = set(stopwords.words('english')) # creating list of stopwords\n",
        "  words = [ w for w in words if not w in stop_words] # creating words list after removing stop words\n",
        "  tweet_lines_train.append(words) # appending those words to created list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DyOhIHI_3X3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# applying above same functions on test data\n",
        "for line in lines_test:\n",
        "  tokens = word_tokenize(line) #tokenizing the words\n",
        "  tokens = [w.lower() for w in tokens]# converting all words to lower case\n",
        "  punc = str.maketrans('', '', string.punctuation) # converting punctuations to none\n",
        "  stripped = [w.translate(punc) for w in tokens]# stripping words\n",
        "  words = [ word for word in stripped if word.isalpha()]# checking if word is alphabets or not\n",
        "  stop_words = set(stopwords.words('english')) # creating list of stopwords\n",
        "  words = [ w for w in words if not w in stop_words]# creating words list after removing stop words\n",
        "  tweet_lines_test.append(words)# appending those words to created list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRMHCbfx7KyG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cc1e2db6-a754-4f24-85ac-402e25b8435c"
      },
      "source": [
        "# printimg length of tweets of train set\n",
        "len(tweet_lines_train)"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "167529"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5WYug0sCw5d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64efa057-b8cb-4112-c9ab-9f5f5a25483e"
      },
      "source": [
        "# printing length of tweets of test set\n",
        "len(tweet_lines_test)"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55843"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Dz3ZaZ67LJ3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "03350461-32ce-4e75-86c1-3e13679d78d1"
      },
      "source": [
        "# importing genism model to apply word2vec\n",
        "import gensim\n",
        "model = gensim.models.Word2Vec(sentences = tweet_lines_train, workers = 4, size = 100) # passing list of tweets that we created above to the model\n",
        "words = list(model.wv.vocab)\n",
        "print(len(words))"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvxwxJdFGvgi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "099255a9-c34c-4ec1-bc3a-d4c277dbf63e"
      },
      "source": [
        "model['good'] # printing vector representation of word good"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.09488058,  0.14240064,  0.9257356 , -0.2774913 , -1.0046504 ,\n",
              "       -0.94951284, -0.29083833,  0.8891745 , -1.4607651 , -1.442384  ,\n",
              "        1.2564552 , -0.9429087 , -2.3215823 , -0.23774625, -1.1593915 ,\n",
              "       -2.2529566 , -2.1913712 , -0.35484153, -0.32041764,  0.8301951 ,\n",
              "        1.0560063 ,  0.81379586,  0.82541084, -2.1180034 , -1.2237502 ,\n",
              "       -1.1063329 ,  0.9364775 ,  0.5382029 , -0.05194172,  0.858803  ,\n",
              "       -0.9182727 ,  0.8142993 , -0.37875992,  1.3758272 ,  0.26651007,\n",
              "        2.018371  , -1.9237154 , -0.91978   ,  1.3341724 , -0.610091  ,\n",
              "        0.8965078 , -1.6427361 , -1.2641482 ,  0.45386913,  2.1333842 ,\n",
              "        0.25273982, -0.832319  ,  1.0900251 ,  0.6217188 ,  0.3691232 ,\n",
              "       -0.67892855,  0.42648834, -1.8846406 , -1.549323  , -0.33330706,\n",
              "        2.638092  , -0.27098605, -0.7238339 ,  0.1440763 , -0.28187224,\n",
              "        1.2893581 ,  1.561416  ,  0.69859403, -0.524405  ,  1.8945317 ,\n",
              "        0.16541326, -0.06328535,  0.5540277 ,  0.2964266 , -0.08321218,\n",
              "        0.38802657, -1.1236509 ,  0.59693474, -2.1375363 ,  1.3388829 ,\n",
              "       -2.1394207 ,  0.05060044,  2.500883  ,  1.8123065 ,  0.19730668,\n",
              "        0.70104676,  1.0533786 , -0.47901246, -0.21862239, -1.7894299 ,\n",
              "        1.1167394 ,  0.40009353,  1.0411227 ,  0.23659822, -0.08429217,\n",
              "       -0.7385466 , -0.5166827 ,  0.26417327,  0.44426295,  1.762628  ,\n",
              "       -1.3861648 ,  0.98888594,  0.19381937, -1.3062065 , -1.2658904 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKsRRQ02B8vL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "23f6151f-4f81-45f9-8a06-524c84735450"
      },
      "source": [
        "#model.wv.syn0"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3.2814380e-03, -8.8560528e-01, -1.7880061e-01, ...,\n",
              "        -3.2024658e+00, -1.3733677e+00,  2.1517618e-01],\n",
              "       [-1.4430740e+00,  1.3881652e+00, -2.3609892e-01, ...,\n",
              "        -5.7840127e-01,  1.0842164e-01,  3.2258648e-01],\n",
              "       [ 6.7940630e-02,  7.5197321e-01,  2.6384523e+00, ...,\n",
              "        -1.2210764e+00, -4.6356502e-01,  1.0725466e+00],\n",
              "       ...,\n",
              "       [-4.2670019e-02,  4.9237370e-02,  1.0883017e-03, ...,\n",
              "        -4.3781451e-03, -1.3166773e-02,  1.7828239e-02],\n",
              "       [-5.6692525e-03,  3.0027037e-02,  9.2792949e-03, ...,\n",
              "        -3.2877803e-02, -2.9108867e-02,  7.2778417e-03],\n",
              "       [-5.8266740e-02,  3.8323585e-02,  2.3869639e-02, ...,\n",
              "         2.1105003e-02, -2.1350157e-02, -3.6782838e-02]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDCLDu5U43ak",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "a66e20a6-192e-4b91-cf69-2d9ebaa2eec0"
      },
      "source": [
        "model.wv.most_similar('trump') # printing words similar to trump"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('donald', 0.8760135173797607),\n",
              " ('president', 0.8714370727539062),\n",
              " ('supporters', 0.8669288158416748),\n",
              " ('hillary', 0.8490664958953857),\n",
              " ('republican', 0.8336113691329956),\n",
              " ('elected', 0.8259866833686829),\n",
              " ('clinton', 0.8180266618728638),\n",
              " ('voters', 0.7988657355308533),\n",
              " ('votes', 0.7851609587669373),\n",
              " ('voted', 0.7791755199432373)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw6GhMA76z1J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "a475227b-8e18-4130-8195-b0ed9b5c2ce9"
      },
      "source": [
        "model.wv.most_similar('corona') # printing words similar to corona"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('bad', 0.7655016779899597),\n",
              " ('great', 0.6394116878509521),\n",
              " ('terrible', 0.5900849103927612),\n",
              " ('best', 0.5495654344558716),\n",
              " ('fantastic', 0.5202822685241699),\n",
              " ('decent', 0.4939470887184143),\n",
              " ('perfect', 0.49374642968177795),\n",
              " ('tough', 0.4932825267314911),\n",
              " ('matters', 0.4841466546058655),\n",
              " ('poor', 0.4780281186103821)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-rBmTFZ2oqC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d9ca36d-1e4e-44fd-84f9-76f9b359db2b"
      },
      "source": [
        "# finding similarities between given 2 words\n",
        "model.wv.similarity('good', 'best')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5495654"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlW9-i315XPY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "edc12ab4-05e2-4c67-ea92-09fa6c8a4a25"
      },
      "source": [
        "model.wv.similarity('virus', 'game')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.19189006"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vbc2oO04JAM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88d5608d-c77b-4506-8a87-c70b6effc8ba"
      },
      "source": [
        "model.wv.similarity('trump', 'amazon')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.17640732"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76nK5TRH4RWd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "69111bf1-5e42-4e6c-b00b-e5d386e4eff5"
      },
      "source": [
        "model.wv.similarity('trump', 'president')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.87143713"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20mraiCwASiP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5827dd6a-b909-4026-fc6d-c0d7e78fb3b4"
      },
      "source": [
        "# creating same model for test data\n",
        "model1 = gensim.models.Word2Vec(sentences = tweet_lines_test, workers = 4, size = 100)\n",
        "words1 = list(model1.wv.vocab)\n",
        "print(len(words1))"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUw89e6LDFbm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model1.wv.syn0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4MQ74UFASmI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "7b943950-e1a1-4a7c-b839-6f491dda82e5"
      },
      "source": [
        "# finding similar words for the given word on test data\n",
        "model1.wv.most_similar('good')"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('bad', 0.8871698379516602),\n",
              " ('great', 0.8120026588439941),\n",
              " ('terrible', 0.7062646150588989),\n",
              " ('whole', 0.6981465816497803),\n",
              " ('awesome', 0.6790273189544678),\n",
              " ('helps', 0.6772798299789429),\n",
              " ('enjoy', 0.6677002906799316),\n",
              " ('best', 0.6652629375457764),\n",
              " ('easy', 0.6638073921203613),\n",
              " ('lots', 0.6574831008911133)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWo8fpB2ASzB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d9a5a84-fad6-43d6-ec1a-5a883285c112"
      },
      "source": [
        "# finding similarity between 2 given words on test data\n",
        "model1.wv.similarity('trump', 'president')"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9523279"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQVy8H2CEv_s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb3702d7-2e91-469a-8108-9819107c9284"
      },
      "source": [
        "print(len(model.wv.syn0))"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQaOeFaGHbO_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to create document  vectors average also by removing words not present in model's vocabilary\n",
        "def document_to_vector(list_words):\n",
        "    #list_words = [word for word in doc if word in model.wv.vocab]\n",
        "    return np.mean(model[list_words], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAfLe9iBbmxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating sample dataframe to apply the above function on train data\n",
        "sample =pd.DataFrame(df['body'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJeJP-3zb5J5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating new column which has words splitted of each comment\n",
        "sample['word_list']= [i.lower().split() for i in sample['body']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeHlgIz0dxpE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "5283c588-7e8b-4880-c17b-1bf6d51cde0a"
      },
      "source": [
        "# printing dataset\n",
        "sample.head()"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>body</th>\n",
              "      <th>word_list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I've always been taught it emerged from the ea...</td>\n",
              "      <td>[i've, always, been, taught, it, emerged, from...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>As an ECE, my first feeling as \"HEY THAT'S NOT...</td>\n",
              "      <td>[as, an, ece,, my, first, feeling, as, \"hey, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Monday: Drug companies stock dives on good new...</td>\n",
              "      <td>[monday:, drug, companies, stock, dives, on, g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i learned that all hybrids are unfertile i won...</td>\n",
              "      <td>[i, learned, that, all, hybrids, are, unfertil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Well i was wanting to get wasted tonight.  Not...</td>\n",
              "      <td>[well, i, was, wanting, to, get, wasted, tonig...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                body                                          word_list\n",
              "0  I've always been taught it emerged from the ea...  [i've, always, been, taught, it, emerged, from...\n",
              "1  As an ECE, my first feeling as \"HEY THAT'S NOT...  [as, an, ece,, my, first, feeling, as, \"hey, t...\n",
              "2  Monday: Drug companies stock dives on good new...  [monday:, drug, companies, stock, dives, on, g...\n",
              "3  i learned that all hybrids are unfertile i won...  [i, learned, that, all, hybrids, are, unfertil...\n",
              "4  Well i was wanting to get wasted tonight.  Not...  [well, i, was, wanting, to, get, wasted, tonig..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW-7JUB3f8ae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing stopwords to remove it from columns of list of words from sample data\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "sample['word_list']=sample['word_list'].apply(lambda x: [item for item in x if item not in stop_words])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw9kj1tJhqVC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "42582d6b-e921-4f7e-f8c6-51cd240352ed"
      },
      "source": [
        "# printing dataframe after adding new column\n",
        "sample.head()"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>body</th>\n",
              "      <th>word_list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I've always been taught it emerged from the ea...</td>\n",
              "      <td>[i've, always, taught, emerged, earth, impace....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>As an ECE, my first feeling as \"HEY THAT'S NOT...</td>\n",
              "      <td>[ece,, first, feeling, \"hey, that's, not-\", th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Monday: Drug companies stock dives on good new...</td>\n",
              "      <td>[monday:, drug, companies, stock, dives, good,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i learned that all hybrids are unfertile i won...</td>\n",
              "      <td>[learned, hybrids, unfertile, wont, read, clic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Well i was wanting to get wasted tonight.  Not...</td>\n",
              "      <td>[well, wanting, get, wasted, tonight., much, r...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                body                                          word_list\n",
              "0  I've always been taught it emerged from the ea...  [i've, always, taught, emerged, earth, impace....\n",
              "1  As an ECE, my first feeling as \"HEY THAT'S NOT...  [ece,, first, feeling, \"hey, that's, not-\", th...\n",
              "2  Monday: Drug companies stock dives on good new...  [monday:, drug, companies, stock, dives, good,...\n",
              "3  i learned that all hybrids are unfertile i won...  [learned, hybrids, unfertile, wont, read, clic...\n",
              "4  Well i was wanting to get wasted tonight.  Not...  [well, wanting, get, wasted, tonight., much, r..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QqL3OGHTuIv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "d073102f-cd5d-45c4-8ef9-2e45e94e78e4"
      },
      "source": [
        "# creating new column which would have avg vector value of each comment\n",
        "sample['doc_vector'] = sample.word_list.apply(document_vector)"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-230-3c4309b2bdc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doc_vector'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3848\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-125-ea152dbd7847>\u001b[0m in \u001b[0;36mdocument_vector\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Create document vectors by averaging word vectors. Remove out-of-vocabulary words.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#doc = [word for word in doc if word in model.wv.vocab]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1420\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m                 )\n\u001b[0;32m-> 1422\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \"\"\"\n\u001b[0;32m-> 1103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.__contains__() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"word 'i've' not in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzCv6hHQbVXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAfyZWoMASrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# installing spacy and other packages to use word2vec from spacy library\n",
        "#!pip install spacy\n",
        "#!python -m spacy download en_core_web_lg\n",
        "#!python -m spacy download en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJnVCVQkASfP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "491e296e-70ba-4053-912e-6931a7ff15c1"
      },
      "source": [
        "'''import spacy\n",
        "#import en_core_web_lg\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "docs_train = [nlp(d.decode()).vector for d in X_train]\n",
        "X_train = np.vstack(docs_train)\n",
        "X_train.shape'''"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import spacy\\n#import en_core_web_lg\\nnlp = spacy.load(\"en_core_web_lg\")\\ndocs_train = [nlp(d.decode()).vector for d in X_train]\\nX_train = np.vstack(docs_train)\\nX_train.shape'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNQAFAgGbDiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}